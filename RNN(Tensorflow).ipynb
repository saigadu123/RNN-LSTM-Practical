{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s_z0DEGAEpWo"
   },
   "source": [
    "# Anna KaRNNa\n",
    "\n",
    "We will build a character-wise RNN trained on Anna Karenina. It'll be able to generate new text based on the text from the book.\n",
    "\n",
    "<img src=\"http://karpathy.github.io/assets/rnn/charseq.jpeg\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K613cO_SEpWp"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import namedtuple\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ic2RUo52EpWw"
   },
   "source": [
    "First we'll load the text file and convert it into integers for our network to use. Here I'm creating a couple dictionaries to convert the characters to and from integers. Encoding the characters as integers makes it easier to use as input in the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BWYik6ZVEpWw"
   },
   "outputs": [],
   "source": [
    "with open('anna.txt', 'r') as f:\n",
    "    text=f.read()\n",
    "vocab = set(text)\n",
    "vocab_to_int = {c: i for i, c in enumerate(vocab)}\n",
    "int_to_vocab = dict(enumerate(vocab))\n",
    "chars = np.array([vocab_to_int[c] for c in text], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V703_bCUEpW1"
   },
   "source": [
    "Let's check out the first 100 characters, make sure everything is peachy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "okndvybaEpW2",
    "outputId": "7c519201-54af-47a1-b385-0d7e06a723fa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chapter 1\\n\\n\\nHappy families are all alike; every unhappy family is unhappy in its own\\nway.\\n\\nEverythin'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5KBvtDD6EpW7"
   },
   "source": [
    "And we can see the characters encoded as integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IXX26PWaEpW8",
    "outputId": "65c0ab7d-0521-4c7c-99e0-347d3db073b6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([77, 25,  1, 43, 35,  3, 29, 69, 33, 74, 74, 74, 71,  1, 43, 43, 37,\n",
       "       69, 67,  1, 11, 39, 36, 39,  3,  2, 69,  1, 29,  3, 69,  1, 36, 36,\n",
       "       69,  1, 36, 39, 72,  3, 38, 69,  3, 16,  3, 29, 37, 69, 49, 28, 25,\n",
       "        1, 43, 43, 37, 69, 67,  1, 11, 39, 36, 37, 69, 39,  2, 69, 49, 28,\n",
       "       25,  1, 43, 43, 37, 69, 39, 28, 69, 39, 35,  2, 69, 27, 68, 28, 74,\n",
       "       68,  1, 37, 63, 74, 74, 21, 16,  3, 29, 37, 35, 25, 39, 28],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FG7Y_cJkEpXA"
   },
   "source": [
    "Since the network is working with individual characters, it's similar to a classification problem in which we are trying to predict the next character from the previous text.  Here's how many 'classes' our network has to pick from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cbmesPbjEpXA",
    "outputId": "2244373d-f17d-4ef1-edb5-431fd4184849"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int32(83)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(chars)+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_eKkopoLEpXE"
   },
   "source": [
    "## Making training and validation batches\n",
    "\n",
    "Now I need to split up the data into batches, and into training and validation sets. I should be making a test set here, but I'm not going to worry about that. My test will be if the network can generate new text.\n",
    "\n",
    "Here I'll make both input and target arrays. The targets are the same as the inputs, except shifted one character over. I'll also drop the last bit of data so that I'll only have completely full batches.\n",
    "\n",
    "The idea here is to make a 2D matrix where the number of rows is equal to the batch size. Each row will be one long concatenated string from the character data. We'll split this data into a training set and validation set using the `split_frac` keyword. This will keep 90% of the batches in the training set, the other 10% in the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6CidnLd9EpXF"
   },
   "outputs": [],
   "source": [
    "def split_data(chars, batch_size, num_steps, split_frac=0.9):\n",
    "    \"\"\" \n",
    "    Split character data into training and validation sets, inputs and targets for each set.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    chars: character array\n",
    "    batch_size: Size of examples in each of batch\n",
    "    num_steps: Number of sequence steps to keep in the input and pass to the network\n",
    "    split_frac: Fraction of batches to keep in the training set\n",
    "    \n",
    "    \n",
    "    Returns train_x, train_y, val_x, val_y\n",
    "    \"\"\"\n",
    "    \n",
    "    slice_size = batch_size * num_steps\n",
    "    n_batches = int(len(chars) / slice_size)\n",
    "    \n",
    "    # Drop the last few characters to make only full batches\n",
    "    x = chars[: n_batches*slice_size]\n",
    "    y = chars[1: n_batches*slice_size + 1]\n",
    "    \n",
    "    # Split the data into batch_size slices, then stack them into a 2D matrix \n",
    "    x = np.stack(np.split(x, batch_size))\n",
    "    y = np.stack(np.split(y, batch_size))\n",
    "    \n",
    "    # Now x and y are arrays with dimensions batch_size x n_batches*num_steps\n",
    "    \n",
    "    # Split into training and validation sets, keep the first split_frac batches for training\n",
    "    split_idx = int(n_batches*split_frac)\n",
    "    train_x, train_y= x[:, :split_idx*num_steps], y[:, :split_idx*num_steps]\n",
    "    val_x, val_y = x[:, split_idx*num_steps:], y[:, split_idx*num_steps:]\n",
    "    \n",
    "    return train_x, train_y, val_x, val_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hqpvGCGGEpXL"
   },
   "source": [
    "Now I'll make my data sets and we can check out what's going on here. Here I'm going to use a batch size of 10 and 50 sequence steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RGqqp0axEpXN"
   },
   "outputs": [],
   "source": [
    "train_x, train_y, val_x, val_y = split_data(chars, 10, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qm4BLeOBEpXQ",
    "outputId": "c16ddb7c-5251-47db-bad3-01407ca088dc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 178650)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zSLBJAeFEpXW"
   },
   "source": [
    "Looking at the size of this array, we see that we have rows equal to the batch size. When we want to get a batch out of here, we can grab a subset of this array that contains all the rows but has a width equal to the number of steps in the sequence. The first batch looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CTWMCdETEpXX",
    "outputId": "1ba64e00-b152-46b3-a4e2-b92853c4bf83"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[77, 25,  1, 43, 35,  3, 29, 69, 33, 74, 74, 74, 71,  1, 43, 43,\n",
       "        37, 69, 67,  1, 11, 39, 36, 39,  3,  2, 69,  1, 29,  3, 69,  1,\n",
       "        36, 36, 69,  1, 36, 39, 72,  3, 38, 69,  3, 16,  3, 29, 37, 69,\n",
       "        49, 28],\n",
       "       [69,  1, 11, 69, 28, 27, 35, 69, 52, 27, 39, 28, 52, 69, 35, 27,\n",
       "        69,  2, 35,  1, 37, 22, 10, 69,  1, 28,  2, 68,  3, 29,  3, 47,\n",
       "        69, 12, 28, 28,  1, 22, 69,  2, 11, 39, 36, 39, 28, 52, 22, 69,\n",
       "        32, 49],\n",
       "       [16, 39, 28, 63, 74, 74, 10, 78,  3,  2, 22, 69, 39, 35, 18,  2,\n",
       "        69,  2,  3, 35, 35, 36,  3, 47, 63, 69,  8, 25,  3, 69, 43, 29,\n",
       "        39, 34,  3, 69, 39,  2, 69, 11,  1, 52, 28, 39, 67, 39, 34,  3,\n",
       "        28, 35],\n",
       "       [28, 69, 47, 49, 29, 39, 28, 52, 69, 25, 39,  2, 69, 34, 27, 28,\n",
       "        16,  3, 29,  2,  1, 35, 39, 27, 28, 69, 68, 39, 35, 25, 69, 25,\n",
       "        39,  2, 74, 32, 29, 27, 35, 25,  3, 29, 69, 68,  1,  2, 69, 35,\n",
       "        25, 39],\n",
       "       [69, 39, 35, 69, 39,  2, 22, 69,  2, 39, 29, 66, 10, 69,  2,  1,\n",
       "        39, 47, 69, 35, 25,  3, 69, 27, 36, 47, 69, 11,  1, 28, 22, 69,\n",
       "        52,  3, 35, 35, 39, 28, 52, 69, 49, 43, 22, 69,  1, 28, 47, 74,\n",
       "        34, 29],\n",
       "       [69, 40, 35, 69, 68,  1,  2, 74, 27, 28, 36, 37, 69, 68, 25,  3,\n",
       "        28, 69, 35, 25,  3, 69,  2,  1, 11,  3, 69,  3, 16,  3, 28, 39,\n",
       "        28, 52, 69, 25,  3, 69, 34,  1, 11,  3, 69, 35, 27, 69, 35, 25,\n",
       "         3, 39],\n",
       "       [25,  3, 28, 69, 34, 27, 11,  3, 69, 67, 27, 29, 69, 11,  3, 22,\n",
       "        10, 69,  2, 25,  3, 69,  2,  1, 39, 47, 22, 69,  1, 28, 47, 69,\n",
       "        68,  3, 28, 35, 69, 32,  1, 34, 72, 69, 39, 28, 35, 27, 69, 35,\n",
       "        25,  3],\n",
       "       [38, 69, 32, 49, 35, 69, 28, 27, 68, 69,  2, 25,  3, 69, 68, 27,\n",
       "        49, 36, 47, 69, 29,  3,  1, 47, 39, 36, 37, 69, 25,  1, 16,  3,\n",
       "        69,  2,  1, 34, 29, 39, 67, 39, 34,  3, 47, 22, 69, 28, 27, 35,\n",
       "        69, 11],\n",
       "       [35, 69, 39,  2, 28, 18, 35, 63, 69,  8, 25,  3, 37, 18, 29,  3,\n",
       "        69, 43, 29, 27, 43, 29, 39,  3, 35, 27, 29,  2, 69, 27, 67, 69,\n",
       "         1, 69,  2, 27, 29, 35, 22, 74, 32, 49, 35, 69, 68,  3, 18, 29,\n",
       "         3, 69],\n",
       "       [69,  2,  1, 39, 47, 69, 35, 27, 69, 25,  3, 29,  2,  3, 36, 67,\n",
       "        22, 69,  1, 28, 47, 69, 32,  3, 52,  1, 28, 69,  1, 52,  1, 39,\n",
       "        28, 69, 67, 29, 27, 11, 69, 35, 25,  3, 69, 32,  3, 52, 39, 28,\n",
       "        28, 39]], dtype=int32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x[:,:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OSjjjviQEpXg"
   },
   "source": [
    "I'll write another function to grab batches out of the arrays made by `split_data`. Here each batch will be a sliding window on these arrays with size `batch_size X num_steps`. For example, if we want our network to train on a sequence of 100 characters, `num_steps = 100`. For the next batch, we'll shift this window the next sequence of `num_steps` characters. In this way we can feed batches to the network and the cell states will continue through on each batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FJDtR5-qEpXi"
   },
   "outputs": [],
   "source": [
    "def get_batch(arrs, num_steps):\n",
    "    batch_size, slice_size = arrs[0].shape\n",
    "    \n",
    "    n_batches = int(slice_size/num_steps)\n",
    "    for b in range(n_batches):\n",
    "        yield [x[:, b*num_steps: (b+1)*num_steps] for x in arrs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8mn8kXBREpXo"
   },
   "source": [
    "## Building the model\n",
    "\n",
    "Below is a function where I build the graph for the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0mEKB2r8EpXq"
   },
   "outputs": [],
   "source": [
    "def build_rnn(num_classes, batch_size=50, num_steps=50, lstm_size=128, num_layers=2,\n",
    "              learning_rate=0.001, grad_clip=5, sampling=False):\n",
    "    \n",
    "    # When we're using this network for sampling later, we'll be passing in\n",
    "    # one character at a time, so providing an option for that\n",
    "    if sampling == True:\n",
    "        batch_size, num_steps = 1, 1\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    # Declare placeholders we'll feed into the graph\n",
    "    inputs = tf.placeholder(tf.int32, [batch_size, num_steps], name='inputs')\n",
    "    targets = tf.placeholder(tf.int32, [batch_size, num_steps], name='targets')\n",
    "    \n",
    "    # Keep probability placeholder for drop out layers\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    \n",
    "    # One-hot encoding the input and target characters\n",
    "    x_one_hot = tf.one_hot(inputs, num_classes)\n",
    "    y_one_hot = tf.one_hot(targets, num_classes)\n",
    "\n",
    "    ### Build the RNN layers\n",
    "    # Use a basic LSTM cell\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "    \n",
    "    # Add dropout to the cell\n",
    "    drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "    \n",
    "    # Stack up multiple LSTM layers, for deep learning\n",
    "#     stacked_rnn = []\n",
    "#         for iiLyr in range():\n",
    "#             stacked_rnn.append(tf.nn.rnn_cell.LSTMCell(num_units=lstm_size, state_is_tuple=True))\n",
    "    \n",
    "    cell = tf.contrib.rnn.MultiRNNCell([drop] * num_layers)\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "    ### Run the data through the RNN layers\n",
    "    # This makes a list where each element is on step in the sequence\n",
    "    rnn_inputs = [tf.squeeze(i, squeeze_dims=[1]) for i in tf.split(x_one_hot, num_steps, 1)]\n",
    "    \n",
    "    # Run each sequence step through the RNN and collect the outputs\n",
    "    outputs, state = tf.contrib.rnn.static_rnn(cell, rnn_inputs, initial_state=initial_state)\n",
    "    final_state = state\n",
    "    \n",
    "    # Reshape output so it's a bunch of rows, one output row for each step for each batch\n",
    "    seq_output = tf.concat(outputs, axis=1)\n",
    "    output = tf.reshape(seq_output, [-1, lstm_size])\n",
    "    \n",
    "    # Now connect the RNN outputs to a softmax layer\n",
    "    with tf.variable_scope('softmax'):\n",
    "        softmax_w = tf.Variable(tf.truncated_normal((lstm_size, num_classes), stddev=0.1))\n",
    "        softmax_b = tf.Variable(tf.zeros(num_classes))\n",
    "    \n",
    "    # Since output is a bunch of rows of RNN cell outputs, logits will be a bunch\n",
    "    # of rows of logit outputs, one for each step and batch\n",
    "    logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "    \n",
    "    # Use softmax to get the probabilities for predicted characters\n",
    "    preds = tf.nn.softmax(logits, name='predictions')\n",
    "    \n",
    "    # Reshape the targets to match the logits\n",
    "    y_reshaped = tf.reshape(y_one_hot, [-1, num_classes])\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_reshaped)\n",
    "    cost = tf.reduce_mean(loss)\n",
    "\n",
    "    # Optimizer for training, using gradient clipping to control exploding gradients\n",
    "    tvars = tf.trainable_variables()\n",
    "    grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars), grad_clip)\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate)\n",
    "    optimizer = train_op.apply_gradients(zip(grads, tvars))\n",
    "    \n",
    "    # Export the nodes\n",
    "    # NOTE: I'm using a namedtuple here because I think they are cool\n",
    "    export_nodes = ['inputs', 'targets', 'initial_state', 'final_state',\n",
    "                    'keep_prob', 'cost', 'preds', 'optimizer']\n",
    "    Graph = namedtuple('Graph', export_nodes)\n",
    "    local_dict = locals()\n",
    "    graph = Graph(*[local_dict[each] for each in export_nodes])\n",
    "    \n",
    "    return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HfSgTtkJEpXx"
   },
   "source": [
    "## Hyperparameters\n",
    "\n",
    "Here I'm defining the hyperparameters for the network. \n",
    "\n",
    "* `batch_size` - Number of sequences running through the network in one pass.\n",
    "* `num_steps` - Number of characters in the sequence the network is trained on. Larger is better typically, the network will learn more long range dependencies. But it takes longer to train. 100 is typically a good number here.\n",
    "* `lstm_size` - The number of units in the hidden layers.\n",
    "* `num_layers` - Number of hidden LSTM layers to use\n",
    "* `learning_rate` - Learning rate for training\n",
    "* `keep_prob` - The dropout keep probability when training. If you're network is overfitting, try decreasing this.\n",
    "\n",
    "Here's some good advice from Andrej Karpathy on training the network. I'm going to write it in here for your benefit, but also link to [where it originally came from](https://github.com/karpathy/char-rnn#tips-and-tricks).\n",
    "\n",
    "> ## Tips and Tricks\n",
    "\n",
    ">### Monitoring Validation Loss vs. Training Loss\n",
    ">If you're somewhat new to Machine Learning or Neural Networks it can take a bit of expertise to get good models. The most important quantity to keep track of is the difference between your training loss (printed during training) and the validation loss (printed once in a while when the RNN is run on the validation data (by default every 1000 iterations)). In particular:\n",
    "\n",
    "> - If your training loss is much lower than validation loss then this means the network might be **overfitting**. Solutions to this are to decrease your network size, or to increase dropout. For example you could try dropout of 0.5 and so on.\n",
    "> - If your training/validation loss are about equal then your model is **underfitting**. Increase the size of your model (either number of layers or the raw number of neurons per layer)\n",
    "\n",
    "> ### Approximate number of parameters\n",
    "\n",
    "> The two most important parameters that control the model are `lstm_size` and `num_layers`. I would advise that you always use `num_layers` of either 2/3. The `lstm_size` can be adjusted based on how much data you have. The two important quantities to keep track of here are:\n",
    "\n",
    "> - The number of parameters in your model. This is printed when you start training.\n",
    "> - The size of your dataset. 1MB file is approximately 1 million characters.\n",
    "\n",
    ">These two should be about the same order of magnitude. It's a little tricky to tell. Here are some examples:\n",
    "\n",
    "> - I have a 100MB dataset and I'm using the default parameter settings (which currently print 150K parameters). My data size is significantly larger (100 mil >> 0.15 mil), so I expect to heavily underfit. I am thinking I can comfortably afford to make `lstm_size` larger.\n",
    "> - I have a 10MB dataset and running a 10 million parameter model. I'm slightly nervous and I'm carefully monitoring my validation loss. If it's larger than my training loss then I may want to try to increase dropout a bit and see if that helps the validation loss.\n",
    "\n",
    "> ### Best models strategy\n",
    "\n",
    ">The winning strategy to obtaining very good models (if you have the compute time) is to always err on making the network larger (as large as you're willing to wait for it to compute) and then try different dropout values (between 0,1). Whatever model has the best validation performance (the loss, written in the checkpoint filename, low is good) is the one you should use in the end.\n",
    "\n",
    ">It is very common in deep learning to run many different models with many different hyperparameter settings, and in the end take whatever checkpoint gave the best validation performance.\n",
    "\n",
    ">By the way, the size of your training and validation splits are also parameters. Make sure you have a decent amount of data in your validation set or otherwise the validation performance will be noisy and not very informative.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JblY0l8zEpXy"
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "num_steps = 100 \n",
    "lstm_size = 512\n",
    "num_layers = 2\n",
    "learning_rate = 0.001\n",
    "keep_prob = 10.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QQyGessrEpX4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A subdirectory or file checkpoints already exists.\n"
     ]
    }
   ],
   "source": [
    "!mkdir checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l9SkzwoZEpX-"
   },
   "source": [
    "## Training\n",
    "\n",
    "Time for training which is pretty straightforward. Here I pass in some data, and get an LSTM state back. Then I pass that state back in to the network so the next batch can continue the state from the previous batch. And every so often (set by `save_every_n`) I calculate the validation loss and save a checkpoint.\n",
    "\n",
    "Here I'm saving checkpoints with the format\n",
    "\n",
    "`i{iteration number}_l{# hidden layer units}_v{validation loss}.ckpt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "foB5jui2EpYA"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'reset_default_graph'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m save_every_n = \u001b[32m200\u001b[39m\n\u001b[32m      4\u001b[39m train_x, train_y, val_x, val_y = split_data(chars, batch_size, num_steps)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m model = \u001b[43mbuild_rnn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m                  \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m                  \u001b[49m\u001b[43mnum_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m                  \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m                  \u001b[49m\u001b[43mlstm_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlstm_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m                  \u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m saver = tf.train.Saver(max_to_keep=\u001b[32m100\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mbuild_rnn\u001b[39m\u001b[34m(num_classes, batch_size, num_steps, lstm_size, num_layers, learning_rate, grad_clip, sampling)\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sampling == \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m      7\u001b[39m     batch_size, num_steps = \u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[43mtf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreset_default_graph\u001b[49m()\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Declare placeholders we'll feed into the graph\u001b[39;00m\n\u001b[32m     12\u001b[39m inputs = tf.placeholder(tf.int32, [batch_size, num_steps], name=\u001b[33m'\u001b[39m\u001b[33minputs\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: module 'tensorflow' has no attribute 'reset_default_graph'"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "# Save every N iterations\n",
    "save_every_n = 200\n",
    "train_x, train_y, val_x, val_y = split_data(chars, batch_size, num_steps)\n",
    "\n",
    "model = build_rnn(len(vocab), \n",
    "                  batch_size=batch_size,\n",
    "                  num_steps=num_steps,\n",
    "                  learning_rate=learning_rate,\n",
    "                  lstm_size=lstm_size,\n",
    "                  num_layers=num_layers)\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "erLbyWvJEpYR",
    "outputId": "a804d5eb-0092-4d3b-d2d9-8c2a5cde9042"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'Session'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mSession\u001b[49m() \u001b[38;5;28;01mas\u001b[39;00m sess:\n\u001b[32m      2\u001b[39m     sess.run(tf.global_variables_initializer())\n\u001b[32m      4\u001b[39m     \u001b[38;5;66;03m# Use the line below to load a checkpoint and resume training\u001b[39;00m\n\u001b[32m      5\u001b[39m     \u001b[38;5;66;03m#saver.restore(sess, 'checkpoints/______.ckpt')\u001b[39;00m\n",
      "\u001b[31mAttributeError\u001b[39m: module 'tensorflow' has no attribute 'Session'"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Use the line below to load a checkpoint and resume training\n",
    "    #saver.restore(sess, 'checkpoints/______.ckpt')\n",
    "    \n",
    "    n_batches = int(train_x.shape[1]/num_steps)\n",
    "    iterations = n_batches * epochs\n",
    "    for e in range(epochs):\n",
    "        \n",
    "        # Train network\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        loss = 0\n",
    "        for b, (x, y) in enumerate(get_batch([train_x, train_y], num_steps), 1):\n",
    "            iteration = e*n_batches + b\n",
    "            start = time.time()\n",
    "            feed = {model.inputs: x,\n",
    "                    model.targets: y,\n",
    "                    model.keep_prob: keep_prob,\n",
    "                    model.initial_state: new_state}\n",
    "            batch_loss, new_state, _ = sess.run([model.cost, model.final_state, model.optimizer], \n",
    "                                                 feed_dict=feed)\n",
    "            loss += batch_loss\n",
    "            end = time.time()\n",
    "            print('Epoch {}/{} '.format(e+1, epochs),\n",
    "                  'Iteration {}/{}'.format(iteration, iterations),\n",
    "                  'Training loss: {:.4f}'.format(loss/b),\n",
    "                  '{:.4f} sec/batch'.format((end-start)))\n",
    "        \n",
    "            \n",
    "            if (iteration%save_every_n == 0) or (iteration == iterations):\n",
    "                # Check performance, notice dropout has been set to 1\n",
    "                val_loss = []\n",
    "                new_state = sess.run(model.initial_state)\n",
    "                for x, y in get_batch([val_x, val_y], num_steps):\n",
    "                    feed = {model.inputs: x,\n",
    "                            model.targets: y,\n",
    "                            model.keep_prob: 1.,\n",
    "                            model.initial_state: new_state}\n",
    "                    batch_loss, new_state = sess.run([model.cost, model.final_state], feed_dict=feed)\n",
    "                    val_loss.append(batch_loss)\n",
    "\n",
    "                print('Validation loss:', np.mean(val_loss),\n",
    "                      'Saving checkpoint!')\n",
    "                saver.save(sess, \"checkpoints/i{}_l{}_v{:.3f}.ckpt\".format(iteration, lstm_size, np.mean(val_loss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e-bQ849DEpYd"
   },
   "source": [
    "#### Saved checkpoints\n",
    "\n",
    "Read up on saving and loading checkpoints here: https://www.tensorflow.org/programmers_guide/variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y2IqZfaQEpYe",
    "outputId": "fed48c6c-9e13-45ad-ad25-6533c231651e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model_checkpoint_path: \"checkpoints/i3560_l512_v1.189.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i200_l512_v2.380.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i400_l512_v1.937.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i600_l512_v1.698.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i800_l512_v1.544.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i1000_l512_v1.429.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i1200_l512_v1.400.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i1400_l512_v1.294.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i1600_l512_v1.253.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i1800_l512_v1.224.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i2000_l512_v1.205.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i2200_l512_v1.187.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i2400_l512_v1.180.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i2600_l512_v1.174.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i2800_l512_v1.173.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i3000_l512_v1.174.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i3200_l512_v1.183.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i3400_l512_v1.188.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i3560_l512_v1.189.ckpt\""
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.get_checkpoint_state('checkpoints')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UYV-fAwFEpWn"
   },
   "source": [
    "# What are Recurrent Neural Networks\n",
    "\n",
    "\n",
    "The idea behind RNNs is to make use of sequential information. In a traditional neural network we assume that all inputs (and outputs) are independent of each other. But for many tasks that’s a very bad idea. If you want to predict the next word in a sentence you better know which words came before it. RNNs are called recurrent because they perform the same task for every element of a sequence, with the output being depended on the previous computations. Another way to think about RNNs is that they have a “memory” which captures information about what has been calculated so far. In theory RNNs can make use of information in arbitrarily long sequences, but in practice they are limited to looking back only a few steps (more on this later). Here is what a typical RNN looks like:\n",
    "\n",
    "![](http://d3kbpzbmcynnmx.cloudfront.net/wp-content/uploads/2015/09/rnn.jpg)\n",
    "\n",
    "The above diagram shows a RNN being unrolled (or unfolded) into a full network. By unrolling we simply mean that we write out the network for the complete sequence. For example, if the sequence we care about is a sentence of 5 words, the network would be unrolled into a 5-layer neural network, one layer for each word. The formulas that govern the computation happening in a RNN are as follows:\n",
    "\n",
    "   * x_t is the input at time step t. For example, x_1 could be a one-hot vector corresponding to the second word of a sentence.\n",
    "   * s_t is the hidden state at time step t. It’s the “memory” of the network. s_t is calculated based on the previous hidden state and the input at the current step: s_t=f(Ux_t + Ws_{t-1}). The function f usually is a nonlinearity such as tanh or ReLU.  s_{-1}, which is required to calculate the first hidden state, is typically initialized to all zeroes.\n",
    "   * o_t is the output at step t. For example, if we wanted to predict the next word in a sentence it would be a vector of probabilities across our vocabulary. o_t = \\mathrm{softmax}(Vs_t).\n",
    "\n",
    "There are a few things to note here:\n",
    "\n",
    "   * You can think of the hidden state s_t as the memory of the network. s_t captures information about what happened in all the previous time steps. The output at step o_t is calculated solely based on the memory at time t. As briefly mentioned above, it’s a bit more complicated  in practice because s_t typically can’t capture information from too many time steps ago.\n",
    "   * Unlike a traditional deep neural network, which uses different parameters at each layer, a RNN shares the same parameters (U, V, W above) across all steps. This reflects the fact that we are performing the same task at each step, just with different inputs. This greatly reduces the total number of parameters we need to learn.\n",
    "\n",
    "   * The above diagram has outputs at each time step, but depending on the task this may not be necessary. For example, when predicting the sentiment of a sentence we may only care about the final output, not the sentiment after each word. Similarly, we may not need inputs at each time step. The main feature of an RNN is its hidden state, which captures some information about a sequence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UkNSCA1kEpYo"
   },
   "source": [
    "## Sampling\n",
    "\n",
    "Now that the network is trained, we'll can use it to generate new text. The idea is that we pass in a character, then the network will predict the next character. We can use the new one, to predict the next one. And we keep doing this to generate all new text. I also included some functionality to prime the network with some text by passing in a string and building up a state from that.\n",
    "\n",
    "The network gives us predictions for each character. To reduce noise and make things a little less random, I'm going to only choose a new character from the top N most likely characters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qHmI7f-lEpYp"
   },
   "outputs": [],
   "source": [
    "def pick_top_n(preds, vocab_size, top_n=5):\n",
    "    p = np.squeeze(preds)\n",
    "    p[np.argsort(p)[:-top_n]] = 0\n",
    "    p = p / np.sum(p)\n",
    "    c = np.random.choice(vocab_size, 1, p=p)[0]\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D7Vi-53vEpYv"
   },
   "outputs": [],
   "source": [
    "def sample(checkpoint, n_samples, lstm_size, vocab_size, prime=\"The \"):\n",
    "    samples = [c for c in prime]\n",
    "    model = build_rnn(vocab_size, lstm_size=lstm_size, sampling=True)\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, checkpoint)\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        for c in prime:\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0,0] = vocab_to_int[c]\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.preds, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "        c = pick_top_n(preds, len(vocab))\n",
    "        samples.append(int_to_vocab[c])\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            x[0,0] = c\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.preds, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "            c = pick_top_n(preds, len(vocab))\n",
    "            samples.append(int_to_vocab[c])\n",
    "        \n",
    "    return ''.join(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yEd9sJWzEpZC"
   },
   "source": [
    "Here, pass in the path to a checkpoint and sample from the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RrzZCDFqEpZC",
    "outputId": "66ef67a3-61f0-462a-8278-a22964edfbb8",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okfful? But if that stronger too was now a\n",
      "spot to Anna And all these. But which he did not let their hair, the\n",
      "chief plies to his brother the peasants who were annoying or\n",
      "enterance whether they had already been into this angire, and there a\n",
      "different the rights of which he was not induls that it was in\n",
      "complaint on the other appointments and parts in to her at once in\n",
      "the world of the sick man. Stiva ano offered a feeling in his parties\n",
      "at the significance, which was a steanterial and a desire of twinting\n",
      "reconciliar worst. He had to give him, he felt ashamed of smart as those\n",
      "ends, and that their words were sitting in strain. The old\n",
      "priest and Vronsky was at once reflicing a blashed of parnies. And, whose\n",
      "should thatch he might spring the matter of the people of his\n",
      "own rools. The carriage storned two stared who labored on tooth\n",
      "his brother's and together with the shadelost emotion, was a second and\n",
      "both sound followed them. He fancied, and though it was impossible;\n",
      "but.\n",
      "\n",
      "She did not know it.\n",
      "\n",
      "She well down the carriage and began to stay it wad to go away.\n",
      "\n",
      "\"How do you kiss your hand?\" he thought. \"What is this this need to\n",
      "see you and then there's and nurse.\" She rose before a note of the\n",
      "petited of impatien.\n",
      "\n",
      "\"The country is the conversation you're at once,\" said Anna.\n",
      "\n",
      "\"Whele I think it off mind.\"\n",
      "\n",
      "And withdark of the second country with which he went off to her, watching\n",
      "away the right third and the porter's eyso that such decending he was\n",
      "confinued thair they he had his secretary in the memory of his\n",
      "own account of the ministers of hates and by the pavening asse, but simply\n",
      "in the contempt, who could be calved to her that the student had and\n",
      "the death of whom she had been too support and another mother of\n",
      "it. But she was not mining the same time and was struck it in answer\n",
      "and anxious to see his work of a country, with the short of the\n",
      "back. The conclualing of which his brother would have bone so\n",
      "through that it, as though it weire hope in sighs, and he could\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Change the name of latest checkpoint accordingly\n",
    "checkpoint = \"checkpoints/i3560_l512_v1.189.ckpt\"\n",
    "samp = sample(checkpoint, 2000, lstm_size, len(vocab), prime=\"Ok\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wxIIp84OEpZH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: tensorflow\n",
      "Version: 2.19.0\n",
      "Summary: TensorFlow is an open source machine learning framework for everyone.\n",
      "Home-page: https://www.tensorflow.org/\n",
      "Author: Google Inc.\n",
      "Author-email: packages@tensorflow.org\n",
      "License: Apache 2.0\n",
      "Location: C:\\One-Drive\\OneDrive - Tredence\\RNN-LSTM-Practical\\rnnvev\\Lib\\site-packages\n",
      "Requires: absl-py, astunparse, flatbuffers, gast, google-pasta, grpcio, h5py, keras, libclang, ml-dtypes, numpy, opt-einsum, packaging, protobuf, requests, setuptools, six, tensorboard, tensorflow-io-gcs-filesystem, termcolor, typing-extensions, wrapt\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Recurrent Neural Network.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "rnnvev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
